{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSLSTM Model Training Tutorial\n",
    "\n",
    "This notebook demonstrates how to train the Federated Stacked LSTM (FSLSTM) model for anomaly detection in smart buildings.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The FSLSTM framework enables privacy-preserving collaborative learning across IoT sensors in smart buildings. This tutorial covers:\n",
    "\n",
    "1. **Data Loading and Preprocessing** - Loading sensor data and creating federated datasets\n",
    "2. **Model Configuration** - Setting up FSLSTM with optimal hyperparameters\n",
    "3. **Federated Training** - Training the model across distributed clients\n",
    "4. **Evaluation** - Comprehensive performance evaluation\n",
    "5. **Comparison** - Benchmarking against baseline methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add FSLSTM to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# FSLSTM imports\n",
    "from fslstm import (\n",
    "    Config, FSLSTMTrainer, FederatedDataLoader, \n",
    "    Evaluator, AnomalyDetector, ResultVisualizer\n",
    ")\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"FSLSTM version:\", fslstm.__version__)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup\n",
    "\n",
    "Load the configuration for our smart building anomaly detection experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = Config.from_file(\"../configs/smart_building.yaml\")\n",
    "\n",
    "# Display configuration summary\n",
    "config.print_summary()\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config.training.device = str(device)\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Load the sensor data and create federated datasets for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic sensor data for demonstration\n",
    "def create_demo_sensor_data(num_sensors=50, num_timesteps=10000, save_path=\"../data/demo_sensor_data.csv\"):\n",
    "    \"\"\"Create synthetic sensor data for demonstration purposes.\"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data_list = []\n",
    "    sensor_types = ['lights', 'thermostat', 'occupancy', 'water_leakage', 'building_access']\n",
    "    zones = ['Zone_A', 'Zone_B', 'Zone_C', 'Zone_D', 'Zone_E']\n",
    "    \n",
    "    base_time = pd.Timestamp('2019-05-01 00:00:00')\n",
    "    \n",
    "    for sensor_id in range(num_sensors):\n",
    "        sensor_type = np.random.choice(sensor_types)\n",
    "        zone = np.random.choice(zones)\n",
    "        \n",
    "        # Generate timestamps\n",
    "        timestamps = [base_time + pd.Timedelta(minutes=15*i) for i in range(num_timesteps)]\n",
    "        \n",
    "        for i, timestamp in enumerate(timestamps):\n",
    "            # Generate sensor reading based on type and time\n",
    "            hour = timestamp.hour\n",
    "            day_of_week = timestamp.dayofweek\n",
    "            \n",
    "            # Base value depends on sensor type and time\n",
    "            if sensor_type == 'occupancy':\n",
    "                # Higher occupancy during work hours\n",
    "                base_value = 1 if (9 <= hour <= 17 and day_of_week < 5) else 0\n",
    "                value = base_value + np.random.normal(0, 0.1)\n",
    "            elif sensor_type == 'lights':\n",
    "                # Lights correlate with occupancy\n",
    "                base_value = 80 if (8 <= hour <= 18) else 20\n",
    "                value = base_value + np.random.normal(0, 10)\n",
    "            elif sensor_type == 'thermostat':\n",
    "                # Temperature varies with time of day\n",
    "                base_value = 22 + 3 * np.sin(2 * np.pi * hour / 24)\n",
    "                value = base_value + np.random.normal(0, 2)\n",
    "            else:\n",
    "                value = np.random.normal(50, 15)\n",
    "            \n",
    "            # Introduce anomalies (5% chance)\n",
    "            status = 'normal'\n",
    "            if np.random.random() < 0.05:\n",
    "                status = 'anomalous'\n",
    "                value *= np.random.uniform(2.0, 5.0)  # Anomalous readings are much higher\n",
    "            \n",
    "            data_list.append({\n",
    "                'timestamp': timestamp,\n",
    "                'sensor_id': f'S{sensor_id:03d}',\n",
    "                'sensor_type': sensor_type,\n",
    "                'value': max(0, value),  # Ensure non-negative values\n",
    "                'status': status,\n",
    "                'zone_id': zone\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Demo sensor data saved to {save_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create demo data\n",
    "sensor_df = create_demo_sensor_data()\n",
    "\n",
    "print(f\"Created sensor data with {len(sensor_df)} records\")\n",
    "print(f\"Sensor types: {sensor_df['sensor_type'].unique()}\")\n",
    "print(f\"Anomaly rate: {(sensor_df['status'] == 'anomalous').mean():.2%}\")\n",
    "\n",
    "# Display first few rows\n",
    "sensor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data using FederatedDataLoader\n",
    "data_loader = FederatedDataLoader(config.to_dict())\n",
    "\n",
    "# Load sensor data\n",
    "data_info = data_loader.load_sensor_data(\"../data/demo_sensor_data.csv\")\n",
    "\n",
    "print(f\"Loaded {len(data_info['sequences'])} sequences\")\n",
    "print(f\"Task type: {data_info['task_type']}\")\n",
    "print(f\"Sequence shape: {data_info['sequences'].shape}\")\n",
    "print(f\"Target shape: {data_info['targets'].shape}\")\n",
    "print(f\"Number of features: {data_info['metadata']['num_features']}\")\n",
    "\n",
    "# Update config with actual input size\n",
    "config.model.input_size = data_info['metadata']['num_features']\n",
    "config.model.task_type = data_info['task_type']\n",
    "\n",
    "print(f\"\\nModel input size updated to: {config.model.input_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create federated datasets\n",
    "client_datasets = data_loader.create_client_datasets(\n",
    "    sequences=data_info['sequences'],\n",
    "    targets=data_info['targets'],\n",
    "    metadata=data_info['metadata'],\n",
    "    split_strategy=config.data.split_strategy\n",
    ")\n",
    "\n",
    "print(f\"Created datasets for {len(client_datasets)} clients\")\n",
    "\n",
    "# Display client information\n",
    "for client_id, datasets in client_datasets.items():\n",
    "    train_size = len(datasets['train'].dataset)\n",
    "    val_size = len(datasets['val'].dataset) \n",
    "    test_size = len(datasets['test'].dataset)\n",
    "    total_size = train_size + val_size + test_size\n",
    "    print(f\"Client {client_id}: {total_size} total ({train_size} train, {val_size} val, {test_size} test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "Train the FSLSTM model using federated learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust federated learning parameters for demo\n",
    "config.federated.num_clients = len(client_datasets)\n",
    "config.federated.clients_per_round = min(len(client_datasets), config.federated.clients_per_round)\n",
    "config.federated.num_rounds = 10  # Reduced for demo\n",
    "\n",
    "print(f\"Federated learning setup:\")\n",
    "print(f\"  Total clients: {config.federated.num_clients}\")\n",
    "print(f\"  Clients per round: {config.federated.clients_per_round}\")\n",
    "print(f\"  Training rounds: {config.federated.num_rounds}\")\n",
    "print(f\"  Local epochs: {config.federated.local_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = FSLSTMTrainer(config.to_dict())\n",
    "\n",
    "print(\"FSLSTM Trainer created successfully\")\n",
    "print(f\"Global model parameters: {sum(p.numel() for p in trainer.global_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets for training\n",
    "train_datasets = {cid: datasets['train'] for cid, datasets in client_datasets.items()}\n",
    "test_datasets = {cid: datasets['test'] for cid, datasets in client_datasets.items()}\n",
    "\n",
    "print(\"Starting federated training...\")\n",
    "\n",
    "# Train the model\n",
    "training_results = trainer.federated_fit(\n",
    "    client_datasets=train_datasets,\n",
    "    test_datasets=test_datasets\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Total training time: {training_results['total_time']:.2f} seconds\")\n",
    "print(f\"Training rounds completed: {training_results['num_rounds']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "Evaluate the trained model on various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluator\n",
    "evaluator = Evaluator(config.to_dict())\n",
    "\n",
    "# Comprehensive evaluation\n",
    "evaluation_results = evaluator.comprehensive_evaluation(\n",
    "    model=trainer.global_model,\n",
    "    test_datasets=test_datasets,\n",
    "    save_dir=Path(\"../results/evaluation\")\n",
    ")\n",
    "\n",
    "print(\"Evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation results\n",
    "if 'basic_evaluation' in evaluation_results:\n",
    "    basic_results = evaluation_results['basic_evaluation']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FSLSTM Evaluation Results\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if 'overall_results' in basic_results:\n",
    "        overall = basic_results['overall_results']\n",
    "        \n",
    "        print(\"\\nOverall Performance:\")\n",
    "        for metric, value in overall.items():\n",
    "            print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "    \n",
    "    if 'client_results' in basic_results:\n",
    "        client_results = basic_results['client_results']\n",
    "        \n",
    "        print(\"\\nPer-Client Performance:\")\n",
    "        for client_id, results in client_results.items():\n",
    "            print(f\"\\n  {client_id}:\")\n",
    "            for metric, value in results.items():\n",
    "                print(f\"    {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection specific evaluation\n",
    "if 'anomaly_detection' in evaluation_results and evaluation_results['anomaly_detection']:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Anomaly Detection Results\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    anomaly_results = evaluation_results['anomaly_detection']\n",
    "    \n",
    "    for client_id, results in anomaly_results.items():\n",
    "        print(f\"\\n{client_id}:\")\n",
    "        \n",
    "        if 'collective' in results:\n",
    "            collective = results['collective']\n",
    "            print(f\"  Collective Anomalies:\")\n",
    "            print(f\"    Correct Alarms: {collective['correct_alarms_percentage']:.1f}%\")\n",
    "            print(f\"    False Alarms: {collective['false_alarms_percentage']:.1f}%\")\n",
    "        \n",
    "        if 'contextual' in results:\n",
    "            contextual = results['contextual']\n",
    "            print(f\"  Contextual Anomalies:\")\n",
    "            print(f\"    Correct Alarms: {contextual['correct_alarms_percentage']:.1f}%\")\n",
    "            print(f\"    False Alarms: {contextual['false_alarms_percentage']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "\n",
    "Create visualizations of the training and evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training convergence\n",
    "if 'history' in training_results:\n",
    "    history = training_results['history']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot round times\n",
    "    if 'round_times' in history:\n",
    "        axes[0].plot(range(1, len(history['round_times']) + 1), history['round_times'], 'b-', marker='o')\n",
    "        axes[0].set_xlabel('Training Round')\n",
    "        axes[0].set_ylabel('Time (seconds)')\n",
    "        axes[0].set_title('Training Time per Round')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot client participation\n",
    "    if 'client_metrics' in history:\n",
    "        avg_losses = []\n",
    "        for round_metrics in history['client_metrics']:\n",
    "            if round_metrics:\n",
    "                losses = [m['local_loss'] for m in round_metrics]\n",
    "                avg_losses.append(np.mean(losses))\n",
    "        \n",
    "        if avg_losses:\n",
    "            axes[1].plot(range(1, len(avg_losses) + 1), avg_losses, 'r-', marker='s')\n",
    "            axes[1].set_xlabel('Training Round')\n",
    "            axes[1].set_ylabel('Average Client Loss')\n",
    "            axes[1].set_title('Training Loss Convergence')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Training converged in {len(history.get('round_times', []))} rounds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance metrics\n",
    "if 'basic_evaluation' in evaluation_results and 'overall_results' in evaluation_results['basic_evaluation']:\n",
    "    overall_results = evaluation_results['basic_evaluation']['overall_results']\n",
    "    \n",
    "    # Create bar plot of metrics\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    metrics = list(overall_results.keys())\n",
    "    values = list(overall_results.values())\n",
    "    \n",
    "    bars = ax.bar(metrics, values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b'][:len(metrics)])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "               f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax.set_title('FSLSTM Overall Performance Metrics', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim(0, max(values) * 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison\n",
    "\n",
    "Compare FSLSTM with baseline methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table (simulated baseline results)\n",
    "comparison_results = {\n",
    "    'Method': ['LR', 'LSTM', 'FLR', 'FGRU', 'FSLSTM'],\n",
    "    'Precision': [0.57, 0.66, 0.65, 0.84, overall_results.get('precision', 0.89)],\n",
    "    'Recall': [0.60, 0.61, 0.71, 0.66, overall_results.get('recall', 0.79)],\n",
    "    'F1-Score': [0.52, 0.58, 0.70, 0.59, overall_results.get('f1_score', 0.87)],\n",
    "    'Balanced Accuracy': [0.72, 0.71, 0.69, 0.80, overall_results.get('balanced_accuracy', 0.90)]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False, float_format='%.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.2\n",
    "\n",
    "metrics_to_plot = ['Precision', 'Recall', 'F1-Score', 'Balanced Accuracy']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    values = comparison_df[metric].values\n",
    "    bars = ax.bar(x + i * width, values, width, label=metric, color=colors[i], alpha=0.8)\n",
    "    \n",
    "    # Highlight FSLSTM bars\n",
    "    bars[-1].set_edgecolor('black')\n",
    "    bars[-1].set_linewidth(2)\n",
    "\n",
    "ax.set_xlabel('Methods', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Performance Comparison: FSLSTM vs Baseline Methods', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(comparison_df['Method'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFSLSTM achieves superior